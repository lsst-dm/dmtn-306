\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font
\usepackage[right]{lineno}
\usepackage[export]{adjustbox}
\linenumbers

% Use this to disable: \usepackage[disable]{todonotes}
\usepackage{todonotes}

\begin{document}

\title{Data Movement Model for the Vera C. Rubin Observatory}

\input{authors}

\abstract{%
The sky images captured nightly by the camera on the Vera C. Rubin Observatory’s telescope will be processed across facilities on three continents. Data acquisition will occur at the observatory's location on Cerro Pachón in the Andes mountains of Chile. A first copy of the raw image data set is stored at the summit and immediately transmitted via dedicated network links to the archive site and the US Data Facility at SLAC National Laboratory in California. After a brief embargo period, the full dataset is transferred to the UK and French Data Facilities, where a third copy is maintained.

Over its 10-year operational period, beginning in late 2025, annual processing campaigns will be conducted by the three facilities on all images collected to date. Sophisticated algorithms will extract the physical properties of celestial objects from these images, producing science-ready images and catalogs. Data products resulting from these processing campaigns will be sent to SLAC for integration into a consistent Data Release, which will be made available to the scientific community through Data Access Centers in the US and Chile, as well as Independent Data Access Centers elsewhere.

In this paper we present an overall view of how we leverage the tools selected for managing the movement of data among the Rubin processing and serving facilities, including Rucio and FTS3. We will also present the tools we developed to integrate Rucio's data model and Rubin's Data Butler, the software abstraction layer that mediates all access to storage by pipeline tasks that implement science algorithms.
}

\maketitle

\section{Introduction}
\label{introduction}
The Vera C. Rubin Observatory's mission is to explore the universe by conducting the Legacy Survey of Space and Time (LSST), the largest-ever sky survey with an unprecedented wide-field imaging system. The observatory aims to capture deep, high-resolution images of the night sky, mapping the cosmos in order to investigate fundamental questions in astrophysics\cite{Ivezic:2019}.

The sky images captured nightly by the observatory's camera will be processed across facilities on three continents. Data acquisition will occur at the observatory's location on Cerro Pachón in the Andes mountains of Chile. A first copy of the raw image data is stored at the summit and immediately transmitted via dedicated network links to the archive site and the US Data Facility at SLAC National Laboratory in California (see figure \ref{fig:data-facilities}). After a brief embargo period, the full dataset is transferred to the UK and French Data Facilities, where a third copy is maintained.

Over its 10-year operational period, beginning in late 2025, annual processing campaigns will be conducted by the three facilities on all images collected to date. Sophisticated algorithms will extract the physical properties of celestial objects from these images, producing science-ready images and catalogs. Data products resulting from these processing campaigns will be sent to SLAC for integration into a consistent Data Release, which will be made available to the scientific community through Data Access Centers in the US and Chile, as well as Independent Data Access Centers elsewhere.

This paper is structured as follows. In section \ref{section-data-movement-use-cases} we present the main data movement use cases we need to satisfy and section \ref{section-data-movement-tools} presents what tools were selected or developed and how they are composed to implement solutions to those use cases.

\begin{figure}[h]
\includegraphics[width=\textwidth]{images/RubinDataFacilities.png}
\caption{Images flow from the Summit Site, where the telescope is located in Chile, to the Base Site and then to the three Rubin Data Facilities which collectively provide the computational capacity for processing the images taken by the Observatory for the duration of the survey.}
\label{fig:data-facilities}
\end{figure}

\section{Data movement use cases}
\label{section-data-movement-use-cases}

This section presents three distinct use cases for moving data among the data facilities used by the Rubin Observatory.

\subsection{From summit to archive}
\label{summit-to-archive}

The data acquisition system stores each exposure as a set of approximately 200 files, one per sensor on the camera focal plane. Once an exposure is recorded at the summit site, its constituent files are transferred in parallel to an object store at the archive center via the S3 protocol. To optimize these transfers over the international network linking the summit to the SLAC archive site, we employ specialized network connection pooling, keep-alive mechanisms, and TCP tuning.

Given that raw images undergo prompt processing for transient object detection and alert generation, the target end-to-end latency for transferring a single exposure—including data compression and other overheads—is set to seven seconds for four gigabytes of compressed data.

Ancillary data (e.g., telemetry, specialized databases) are replicated to the archive center using native protocols to ensure timeliness. Additionally, a small number of certified calibration files are transferred infrequently from the archive to the summit and other locations.

\subsection{From archive to processing facilities and back}
\label{summit-to-data-facilities}

Annual processing of the entire image dataset recorded since the beginning of the survey is carried out across three facilities: the US Data Facility, hosted at SLAC National Laboratory in California, USA\footnote{https://www.slac.stanford.edu}, the French Data Facility, hosted by the IN2P3 computing center (CC-IN2P3) in Lyon, France\footnote{https://cc.in2p3.fr}, and the UK Data Facility, operated by the LSST:UK consortium\footnote{https://www.lsst.ac.uk}.

Raw image data is replicated from the US to the European facilities. Both the US and French data facilities store a complete copy of the raw image dataset. The UK facility receives the raw images corresponding to the spatial region assigned to it for processing. Data movement between these sites is facilitated by ESnet\footnote{https://es.net}, which handles transatlantic data transport; GEANT\footnote{https://geant.org}, which connects European sites; and the national research and education networks, JANET\footnote{https://www.jisc.ac.uk/janet} (UK) and RENATER\footnote{https://renater.fr} (France).

The entire set of final data products, along with selected intermediate products from each campaign, is replicated from the facility where they are generated to the archive center. There, they are consolidated and incorporated into a new data release, which is delivered annually to the science collaborations for analysis \cite{10.1051_epjconf_202429501042}.

The LSST Science Pipelines is the software developed by Rubin Observatory to process the survey data \cite{bosch-pipelines}. It includes advanced image processing algorithms and supporting middleware. A central component of this middleware is the Rubin Data Butler, an abstraction layer that mediates access to the data required by, or generated through, the pipelines \cite{2022SPIE12189E..11J}. The Data Butler retrieves data from persistent storage (using appropriate protocols and data formats) based on queries specified by scientifically relevant identifiers (rather than file paths), and delivers the data as in-memory Python objects to the pipelines. It also persists the in-memory objects generated by the science algorithms. Crucially, the Butler manages the location of all files within the data store, recording their locations and relationships in a relational database. Together, the file registry and the storage system where files are located constitute a \emph{repository}.

Since the Rubin's data Butler is aware only of the files present at a single facility, files replicated between facilities need to be placed in the repository's data store at the location expected by the Butler. Upon reception, replicated files are ingested into the receiving facility's local Butler repository, making them available for the processing pipelines.


\subsection{From archive to data access centers}
\label{summit-to-data-access-centers}

Annually-released data products must be distributed to approximately 15 to 20 data access centers across the America, Europe, and Asia-Pacific regions, where scientific analysis will be conducted. These distribution campaigns will be centrally coordinated by Rubin to ensure timely delivery of data releases to all analysis centers. The goal is to distribute the multi-petabyte datasets across multiple sites using the network bandwidth available at several participating sites, thereby reducing the load on the archive center \cite{RTN-086}.

\section{Data movement tools}
\label{section-data-movement-tools}

Several software tools are employed to implement the use cases outlined in the previous section. CERN's Rucio \cite{rucio2019} and its companion FTS \cite{FTS} manage the movement of files between the archive site and data facilities, as well as from the archive to the data access centers. In addition, Rubin-specific tools have been developed to register files and automate actions when replicated files arrive at their destination. These tools and their usage are described in the following subsections.

Rubin Observatory operates a dedicated instance of Rucio, configured to transfer files between the Rucio Storage Elements (RSEs) at each facility. These storage endpoints support a data movement protocol that Rucio utilizes to transport data across them. The US and UK data facilities use XrootD \cite{xrootd}, while the French data facility uses dCache \cite{dCache}. All of these systems expose the webDAV protocol \cite{webdav}, an extension of HTTP \cite{http1.1}. Data is transferred securely across sites using confidential channels built on top of secure HTTP.

Each processing facility exposes at least two RSEs: one for storing input data required for processing (e.g., raw images, calibration data, reference catalogs, etc.) and another endpoint for storing the products generated by the image processing pipelines. All RSEs are configured to use the identity logical-to-physical filename mapping. This configuration ensures that the file pathnames are preserved relative to the Butler repository's datastore location, which is critical for proper file replication to the destination where the Butler expects to find them.

\subsection{Registration of files to replicate}
\label{registration}

To perform replication, we create Rucio Datasets, each composed of a set of files that are already in their appropriate locations at the source RSE. Upon registration, pre-configured Rucio subscriptions trigger the actual file movement to the destination facility, in accordance with the defined replication rules. Rucio delegates the data transfer to FTS, which then instructs the storage endpoints at the facilities to move the data, typically by requesting the destination facility to pull the data from the source facility.

Rubin has developed the tool \texttt{rucio\_register}\footnote{https://github.com/lsst/rucio\_register}, which allows for the selection of existing files from a Butler repository based on specified criteria. The tool attaches Rubin-specific metadata to these files and registers them into one or more Rucio Datasets. The metadata, encoded as a JSON record, contains a minimal set of information extracted from the origin Butler repository. This ensures that replicated files can be ingested properly into the local Butler repository at the destination facility.

Only files that require replication to another facility are registered with Rucio. As a result, Rubin's instance of Rucio is aware only of files replicated across processing facilities. Files that are local to each facility and not subject to replication remain known only to that facility's Butler and are not registered in Rucio.

\subsection{Ingestion at reception}
\label{ingestion}

FTS notifies Rucio about the completion of individual file transfers. Rubin's \texttt{HermesK}\footnote{https://github.com/lsst-dm/ctrl\_rucio\_ingest}, which is a modification of Rucio's Hermes daemon, filters messages and uses Kafka as a mechanism to signal the destination Rubin facility that a new file was replicated and to take appropriate actions.

Messages distributed through Rubin's Kafka control-plane include Rubin-specific metadata. Those messages are received by Rubin's \texttt{ingestd}\footnote{https://github.com/lsst-dm/ctrl\_ingestd}, a daemon running at each destination facility responsible for ingesting newly replicated files into the local Butler repository.

Each facility only receives notifications about files successfully replicated to the storage endpoints it operates. This is achieved by following a simple convention: the name of Kafka topic the notification is sent to is named after the name of the Rucio storage element. Each facility's \texttt{ingestd} is configured to only monitor Kafka messages specifically targeted to the facility's RSEs (see figure \ref{fig:kafka-control-plane}).

\begin{figure}[h]
\includegraphics[width=0.9\textwidth, center]{images/HermesAndIngestd.png}
\caption{Notifications about successful file transfers are emitted by \texttt{HermesK} using Kafka topics named after the destination RSE. At the receiving facility \texttt{ingestd} monitors those notifications and ingests the newly received file into the local Butler repository. The JSON-encoded, Rubin-specific metadata associated to the file when it was first registered into Rucio contains the details needed for ingestion.}
\label{fig:kafka-control-plane}
\end{figure}


\section{Summary}
\label{summary}
We presented several use cases for the movement of data among the facilities participating to processing of Rubin Observatory data, the tools used to implement solutions to satisfy those uses cases as well as the tools Rubin has developed for integrating Rubin-specific software components to more generic software systems for large scale inter-site data movement.

\section{Acknowledgments}

This material is based upon work supported in part by the National Science Foundation through Cooperative Agreement AST-1258333 and Cooperative Support Agreement AST-1202910 managed by the Association of Universities for Research in Astronomy (AURA), and the Department of Energy under Contract No. DE-AC02-76SF00515 with the SLAC National Accelerator Laboratory managed by Stanford University. Additional Rubin Observatory funding comes from private donations, grants to universities, and in-kind support from LSSTC Institutional Members.

\bibliography{references}

\end{document}
